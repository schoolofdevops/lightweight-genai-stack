# Lightweight GenAI Stack Configuration

# LLM Model (choose based on your RAM)
# tinyllama:1.1b   - 1.1B params, ~600MB, fastest (recommended for 6GB)
# phi3:mini        - 3.8B params, ~2.3GB, great balance (recommended for 8GB)
# llama3.2:3b      - 3B params, ~2GB, good general purpose
# qwen2.5:3b       - 3B params, ~2GB, good for multilingual
LLM_MODEL=tinyllama:1.1b

# Embedding Model
# nomic-embed-text - ~275MB, good quality embeddings
# all-minilm       - ~45MB, faster but less accurate
EMBEDDING_MODEL=nomic-embed-text

# Ollama Configuration
OLLAMA_BASE_URL=http://ollama:11434

# ChromaDB Configuration
CHROMA_HOST=chromadb
CHROMA_PORT=8000
