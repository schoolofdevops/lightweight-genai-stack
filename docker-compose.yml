services:
  # Docker Model Runner - LLM Service (requires Docker Desktop 4.40+)
  # Uses provider.type: model to run LLM directly on host via llama.cpp
  llm:
    provider:
      type: model
      options:
        model: ai/llama3.2:1B-Q8_0

  # ChromaDB - Lightweight Vector Database
  chromadb:
    image: chromadb/chroma:latest
    container_name: chromadb
    ports:
      - "8000:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    deploy:
      resources:
        limits:
          memory: 512M
    restart: unless-stopped

  # GenAI App - Streamlit RAG Application
  genai-app:
    build:
      context: ./app
      dockerfile: Dockerfile
    container_name: genai-app
    ports:
      - "8501:8501"
    environment:
      # Docker Model Runner exposes OpenAI-compatible API at this endpoint
      - OPENAI_API_BASE=http://model-runner.docker.internal/engines/llama.cpp/v1
      - OPENAI_API_KEY=docker-model-runner
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - LLM_MODEL=ai/llama3.2:1B-Q8_0
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
    depends_on:
      llm:
        condition: service_started
      chromadb:
        condition: service_started
    volumes:
      - ./app:/app
      - uploaded_docs:/app/uploads
    deploy:
      resources:
        limits:
          memory: 2G
    restart: unless-stopped

volumes:
  chroma_data:
  uploaded_docs:

networks:
  default:
    name: genai-network
